Notes for the auto-encoder:

Still a lot of hyperparameter tuning to do. But the model works overall. Will attempt to do a Baysiean
optimaization technique or some of the automatic hyperparameter tuning. Still need to figure out how to
properly adjust the layers such that they reproduce the initial image dimensions for the loss calculations
without me having to manually do some math on the strides and filter sizes to get the dimensions right.

Version Descriptions:
        v0: 	First model that produced results.

                e = Conv2D(filters=64, kernel_size=3, activation='relu', strides=1, padding='same')(model_input)
                e = Conv2D(filters=128, kernel_size=3, activation='relu', strides=1, padding='same')(e)
                e = Conv2D(filters=256, kernel_size=3, activation='relu', strides=1, padding='same')(e)
                shape = int_shape(e)
                e = Flatten()(e)
                bottleneck = Dense(64, name='bottleneck')(e)
                d = Dense(shape[1]*shape[2]*shape[3])(bottleneck)
                d = Reshape((shape[1], shape[2], shape[3]))(d)
                d = Conv2DTranspose(filters=256, kernel_size=3, activation='relu', strides=1, padding='same')(d)
                d = Conv2DTranspose(filters=128, kernel_size=3, activation='relu', strides=1, padding='same')(d)
                d = Conv2DTranspose(filters=64, kernel_size=2, activation='relu', strides=1, padding='same')(d)
                model_outputs = Conv2DTranspose(filters=1, kernel_size=1, activation='linear', padding='same', name='decoder_output')(d)
                full_autoencoder = Model(inputs=model_input, outputs=model_outputs)
                opt = tf.keras.optimizers.Adam(learning_rate=1e-6)
                full_autoencoder.compile(optimizer=opt, loss='mse')

        v1: 	Includes the addition of maxpooling layers in between the initial Conv2D layers. Required adjusting
                of the strides in the decoder layers.

                e = Conv2D(filters=64, kernel_size=3, activation='relu', strides=1, padding='same')(model_input)
                e = MaxPooling2D(pool_size=2, strides=2)(e)
                e = Conv2D(filters=128, kernel_size=3, activation='relu', strides=1, padding='same')(e)
                e = MaxPooling2D(pool_size=3, strides=3)(e)
                e = Conv2D(filters=256, kernel_size=3, activation='relu', strides=1, padding='same')(e)
                shape = int_shape(e)
                e = Flatten()(e)
                bottleneck = Dense(64, name='bottleneck')(e)
                d = Dense(shape[1]*shape[2]*shape[3])(bottleneck)
                d = Reshape((shape[1], shape[2], shape[3]))(d)
                d = Conv2DTranspose(filters=256, kernel_size=3, activation='relu', strides=3, padding='same')(d)
                d = Conv2DTranspose(filters=128, kernel_size=3, activation='relu', strides=2, padding='same')(d)
                d = Conv2DTranspose(filters=64, kernel_size=2, activation='relu', strides=1, padding='same')(d)
                model_outputs = Conv2DTranspose(filters=1, kernel_size=1, activation='linear', padding='same', name='decoder_output')(d)
                full_autoencoder = Model(inputs=model_input, outputs=model_outputs)
                opt = tf.keras.optimizers.Adam(learning_rate=1e-6)
                full_autoencoder.compile(optimizer=opt, loss='mse')

                Antecdotally the inclusion of the maxpooling layers apeared to introduce more noise to the resulting prediction.


        v2:     Goes back to the v0 arcetecture without the maxpooling layers and changes the loss function from mean-squared error
                to binary crossentropy. This is supposidly good for one channel CNN autoencoders since the binary cross entropy will
                output the "probability" of the pixel being black, which jsut sort of works here?

                e = Conv2D(filters=64, kernel_size=3, activation='relu', strides=1, padding='same')(model_input)
                e = Conv2D(filters=128, kernel_size=3, activation='relu', strides=1, padding='same')(e)
                e = Conv2D(filters=256, kernel_size=3, activation='relu', strides=1, padding='same')(e)
                shape = int_shape(e)
                e = Flatten()(e)
                bottleneck = Dense(64, name='bottleneck')(e)
                d = Dense(shape[1]*shape[2]*shape[3])(bottleneck)
                d = Reshape((shape[1], shape[2], shape[3]))(d)
                d = Conv2DTranspose(filters=256, kernel_size=3, activation='relu', strides=1, padding='same')(d)
                d = Conv2DTranspose(filters=128, kernel_size=3, activation='relu', strides=1, padding='same')(d)
                d = Conv2DTranspose(filters=64, kernel_size=3, activation='relu', strides=1, padding='same')(d)
                model_outputs = Conv2DTranspose(filters=1, kernel_size=1, activation='linear', padding='same', name='decoder_output')(d)
                full_autoencoder = Model(inputs=model_input, outputs=model_outputs)
                opt = tf.keras.optimizers.Adam(learning_rate=1e-6)
                full_autoencoder.compile(optimizer=opt, loss='binary_crossentropy')

                Antecdotally the binary crossentropy apeared to make the results worse. Missing some bright spots and elevating the missing data.


        v3:     Same as v0 but with two added layers in both the decoder and encoder. Trained for 500 epochs or until early stopping kicks in.
                Had to trim down the training and val set to half their size to be able to run this on the graphics card.

                e = Conv2D(filters=32, kernel_size=3, activation='relu', strides=1, padding='same')(model_input)
                e = Conv2D(filters=64, kernel_size=3, activation='relu', strides=1, padding='same')(e)
                e = Conv2D(filters=128, kernel_size=3, activation='relu', strides=1, padding='same')(e)
                e = Conv2D(filters=256, kernel_size=3, activation='relu', strides=1, padding='same')(e)
                e = Conv2D(filters=512, kernel_size=3, activation='relu', strides=1, padding='same')(e)
                shape = int_shape(e)
                e = Flatten()(e)
                bottleneck = Dense(64, name='bottleneck')(e)
                d = Dense(shape[1]*shape[2]*shape[3])(bottleneck)
                d = Reshape((shape[1], shape[2], shape[3]))(d)
                d = Conv2DTranspose(filters=512, kernel_size=3, activation='relu', strides=1, padding='same')(d)
                d = Conv2DTranspose(filters=256, kernel_size=3, activation='relu', strides=1, padding='same')(d)
                d = Conv2DTranspose(filters=128, kernel_size=3, activation='relu', strides=1, padding='same')(d)
                d = Conv2DTranspose(filters=64, kernel_size=3, activation='relu', strides=1, padding='same')(d)
                d = Conv2DTranspose(filters=32, kernel_size=3, activation='relu', strides=1, padding='same')(d)
                model_outputs = Conv2DTranspose(filters=1, kernel_size=1, activation='linear', padding='same', name='decoder_output')(d)
                full_autoencoder = Model(inputs=model_input, outputs=model_outputs)
                opt = tf.keras.optimizers.Adam(learning_rate=1e-6)		# learning rate that actually started producing good results
                full_autoencoder.compile(optimizer=opt, loss='mse')					# Ive read that cross entropy is good for this type of model
                early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=early_stopping_patience)		# early stop process prevents overfitting
                full_autoencoder = fit_autoencoder(full_autoencoder, train, val, early_stop)

